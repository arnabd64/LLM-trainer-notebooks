{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain a GPT2 model using Huggingface `Trainer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Environment\n",
    "\n",
    "## 1.1 Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade transformers datasets tensorboard > install.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Connect to Online Services\n",
    "\n",
    "Here you have to connect this notebook to online services like [Huggingface](https://hf.co) or [Weights & Biases](https://wandb.ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface\n",
    "HF_TOKEN = \"Paste-your-token\"\n",
    "\n",
    "huggingface_hub.login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Cache Directory\n",
    "\n",
    "Location of downloaded models, tokenizers and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE = os.path.join(os.getcwd(), \"hf_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenizer & Model\n",
    "\n",
    "## 2.1 Get the GPT2 Tokenizer\n",
    "\n",
    "The scope of this notebook does not include training a Tokenizer and hence we will use a `PretrainedTokenizer` provided by [OpenAI-Community](https://huggingface.co/openai-community)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKENIZER_ID = \"openai-community/gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HF_TOKENIZER_ID, cache_dir=CACHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Instantiate Model\n",
    "\n",
    "Here you have the choice either to __train a GPT2 model from scratch__ or to further train an __existing GPT2 model__  using your own data. This notebook will cover both cases but when you will start the training process, you need to choose either one of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Scratch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scratch_model(vocab_size: int, context_length: int, embedding_dim=768, encoder_layers=12, attention_heads=12):\n",
    "    \"\"\"\n",
    "    Instantiate GPT2Model from scratch.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    - `vocab_size`: Total unique tokens the model can understand.\n",
    "    - `context_length`: Maximum length of the input sequences.\n",
    "    - `embedding_dim`: Dimensionality of Word Embeddings\n",
    "    - `encoder_layers`: Transformer Encoder Layers\n",
    "    - `attention_heads`: Number of Parallel Attention mechanisms in each Encoder Layer.\n",
    "    \"\"\"\n",
    "    # configuartion for the model\n",
    "    config = GPT2Config(\n",
    "        vocab_size=vocab_size,\n",
    "        n_positions=context_length,\n",
    "        n_embd=embedding_dim,\n",
    "        n_layer=encoder_layers,\n",
    "        n_head=attention_heads\n",
    "    )\n",
    "\n",
    "    # instanciate model\n",
    "    model = GPT2LMHeadModel(config)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_model(hf_model_id=\"openai-community/gpt2\"):\n",
    "    \"\"\"\n",
    "    Download a pretrained model from Huggingface.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    - `hf_model_id`: repository id for the pretrained model.\n",
    "    \"\"\"\n",
    "    # download model\n",
    "    model = GPT2LMHeadModel.from_pretrained(hf_model_id, cache_dir=CACHE, token=HF_TOKEN)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset\n",
    "\n",
    "## 3.1 Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = \"HuggingFaceFW/fineweb\"\n",
    "SUBSET = \"default\"\n",
    "TRAINING_SPLIT = \"train\"\n",
    "TEXT_KEY = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.load_dataset(\n",
    "    DATASET_ID,\n",
    "    SUBSET,\n",
    "    split=TRAINING_SPLIT,\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, _tokenizer=tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes mini batches sampled from the dataset.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = _tokenizer(\n",
    "        example[TEXT_KEY],\n",
    "        padding=\"right\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(tokenize, batched=True).select_columns([\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup `Trainer`\n",
    "\n",
    "## 4.1 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple\n",
    "TRAINED_MODEL_NAME = \"gpt2-fineweb\"\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_TRAIN_STEPS = 10000\n",
    "EVALUATION_STEPS = MAX_TRAIN_STEPS // 10\n",
    "SAVE_STEPS = EVALUATION_STEPS // 4\n",
    "SEED = 854\n",
    "\n",
    "# Advanced (Optional)\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "WEIGHT_DECAY = 0\n",
    "LEARNING_RATE_SCHEDULER = \"constant\"\n",
    "WARMUP_RATIO = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=TRAINED_MODEL_NAME,\n",
    "    overwrite_output_dir=True,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVALUATION_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    max_steps=MAX_TRAIN_STEPS,\n",
    "    push_to_hub=True,\n",
    "    hub_token=HF_TOKEN,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_gpu_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    lr_scheduler_type=LEARNING_RATE_SCHEDULER,\n",
    "    seed=SEED,\n",
    "    report_to=[\"tensorboard\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model\n",
    "model = scratch_model(vocab_size=tokenizer.vocab_size, context_length=1024)\n",
    "\n",
    "#  data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir {TRAINED_MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\n",
    "token=HF_TOKEN,\n",
    "tags = [\"pytorch\", trainer.model.__class__.__name__]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
